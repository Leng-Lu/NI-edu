{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing computational models\n",
    "\n",
    "This week's tutorial is about using RSA to assess the explanatory power of *different* computational models in explaining neural processing! These computational models will serve as hypotheses about the computations that are important for neural processing, just like we used the participants behaviour or the experimental conditions in the last tutorial. In particular, we'll be looking at a new fMRI dataset on natural image processing, called **BOLD5000**, we will evaluate two different computational models on one of the ROIs, the lateral occipital cortex (LOC).\n",
    "\n",
    "**What you'll learn**: At the end of this tutorial, you ...\n",
    "\n",
    "* are familiar with noise ceilings and you will know how to compute them.\n",
    "* know how to use a deep convolutional neural network (DCNN) for object recognition\n",
    "* are able to obtain features from a DCNN and convert them into an RDM\n",
    "* can estimate semantic similarity using linguistic word embeddings\n",
    "* can convert word embeddings into an RDM\n",
    "* can compare different candidate RDMs for explaining neural RDMs\n",
    "\n",
    "**Estimated time needed to complete**: 8-12 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports for the rest of the tutorial\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import joblib\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from niedu.utils.nipa import show_rankRDM\n",
    "sys.path.append('/home/Public')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and getting to know BOLD5000\n",
    "In this notebook, we are going to work with the **BOLD5000 dataset**. \n",
    "\n",
    "### Downloading the datasets\n",
    "As a first step, we need to download the images and labels, the participants saw during the experiment. We also need to dowwnload the fMRI data. Conviniently for us, the patterns from a set of ROIs is already available for download, so we'll start from a set of patterns from every hemisphere for 4 participants!\n",
    "\n",
    "The download below will take a long time! So in the meantime, you have time familiarize yourself with the dataset by looking at the [website](https://bold5000.github.io/overview.html) and the [paper](https://www.nature.com/articles/s41597-019-0052-3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(os.path.expanduser('~'), 'NI-edu-data')):\n",
    "    os.mkdir(os.path.expanduser('~'), 'NI-edu-data')\n",
    "\n",
    "data_dir = os.path.join(os.path.expanduser('~'), 'NI-edu-data', 'BOLD5000')\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "img_dir = os.path.join(data_dir, 'BOLD5000_Stimuli')\n",
    "if not os.path.isdir(img_dir):\n",
    "    print(\"Downloading the dataset (+- ... MB) ...\")\n",
    "    !wget -P $data_dir https://www.dropbox.com/s/5ie18t4rjjvsl47/BOLD5000_Stimuli.zip\n",
    "    filename = os.path.join(data_dir, 'BOLD5000_Stimuli.zip')\n",
    "    !unzip $filename -d $data_dir\n",
    "    #!rm $filename\n",
    "    print(\"\\nDone!\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded!\")\n",
    "    \n",
    "if not os.path.isfile(os.path.join(data_dir, 'BOLD_5000_Stimuli', 'Image_Labels', 'coco-labels-paper.txt')):\n",
    "    filename = os.path.join(data_dir, 'BOLD5000_Stimuli', 'Image_Labels', 'coco-labels-paper.txt')\n",
    "    !wget -O $filename https://surfdrive.surf.nl/files/index.php/s/PcJgbhnso6XYcqz/download\n",
    "                      \n",
    "if not os.path.isfile(os.path.join(img_dir, 'Image_Labels', 'scene_final_labels_corrected.txt')):\n",
    "    filename = os.path.join(img_dir, 'Image_Labels', 'scene_final_labels_corrected.txt')\n",
    "    !wget -O $filename https://surfdrive.surf.nl/files/index.php/s/1Wvo7SHx3W22iox/download\n",
    "            \n",
    "roi_dir = os.path.join(data_dir, 'ROIs')\n",
    "if not os.path.isdir(roi_dir):\n",
    "    print(\"Downloading the neural data (+- ... MB) ...\")\n",
    "    filename = os.path.join(data_dir, 'ROIs.zip')\n",
    "    !wget -O $filename https://ndownloader.figshare.com/files/12965447 \n",
    "    !unzip $filename -d $data_dir\n",
    "    #!rm $filename\n",
    "    print(\"\\nDone!\")\n",
    "else:\n",
    "    print(\"Neural data already downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we've downloaded all the data we will need for this notebook! Let's have a look at the neural data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_dir = os.path.join(data_dir, 'ROIs')\n",
    "print(\"We have the following folders:\\n-\", '\\n- '.join(sorted(os.listdir(roi_dir))))\n",
    "\n",
    "sub_dir = os.path.join(data_dir, 'ROIs', 'CSI1','h5')\n",
    "print(\"We have the following folders:\\n-\", '\\n- '.join(sorted(os.listdir(sub_dir))))\n",
    "\n",
    "stim_dir = os.path.join(data_dir, 'ROIs', 'stim_lists')\n",
    "print(\"We have the following folders:\\n-\", '\\n- '.join(sorted(os.listdir(stim_dir))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in every folder there are multiple patterns available averaged for the different TR's. In the next steps, we will be loading in these patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Which TR dataset should we use for our analysis and why? (Hint: Check out the figures in the paper!). Place your TR reply in a variable called TR.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f00f764134654f16b04eee72a265802",
     "grade": false,
     "grade_id": "cell-7849b1fde5dddaf3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab10279eca31e58cd7d623acaa6b4150",
     "grade": true,
     "grade_id": "cell-14203066f8aa85ac",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from week_lynn import test_TR\n",
    "\n",
    "test_TR(TR)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the datasets\n",
    "As a first step, let's load in the pattern for our region of interest LOC for all participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "data = {} # preallocate!\n",
    "roi_key = 'LOC' # defines our ROI!\n",
    "for sub in range(1,5): # to enumerate over subjects!\n",
    "    print('Processing subject ' + str(sub) + ' ...')\n",
    "    data['CSI' + str(sub)]={}\n",
    "    filename = os.path.join(data_dir, 'ROIs', 'CSI' + str(sub),'h5',\"CSI\" + str(sub) + \"_ROIs_TR34\" + \".h5\") \n",
    "    stimname = os.path.join(data_dir, 'ROIs','stim_lists','CSI0' + str(sub) + '_stim_lists.txt') \n",
    "    # reads in the neural patterns\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        for k in f.keys():\n",
    "            if k.endswith(roi_key):\n",
    "                data['CSI' + str(sub)][k]= list(f[k])\n",
    "    # reads in the stimulus information\n",
    "    data['CSI' + str(sub)]['stim'] = open(stimname).read().splitlines()\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, so we've just loaded in all the patterns that we need! As a next step, we need to organize them in the same order, so that we can turn them into an RDM. For convenience, we'll organize the images alphabetically for all subjects. This will create some structure for our RDMS since:\n",
    "* Images from imageNet with n...followed by 8 digits, e.g. n01692333_12353.JPEG\n",
    "* Images from coco contain the string COCO_, e.g. COCO_train2014_000000273147.jpg\n",
    "* Images from the Scene database names such as dinosaur4.jpg\n",
    "\n",
    "Subject 4 has less trials, with only around 3000 so let's focus on these images and discard all other trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ids = pd.DataFrame(np.zeros((len(data['CSI4']['stim']),5)), \n",
    "                         columns = ['ID', 'CSI4_trials', 'CSI1_trials', 'CSI2_trials', 'CSI3_trials'])\n",
    "trial_ids['ID'] = data['CSI4']['stim']\n",
    "trial_ids['CSI4_trials'] = np.arange(len(data['CSI4']['stim']))\n",
    "\n",
    "# Now let's find the corresponding trials for the remaining subjects\n",
    "for i in range(len(trial_ids['ID'])):\n",
    "    for sub in range(1,4):\n",
    "        trial_ids.loc[i, 'CSI' + str(sub) + '_trials'] = int(data['CSI'+ str(sub)]['stim'].index(trial_ids.loc[i,'ID']))\n",
    "\n",
    "# Finally, let's sort all of them alphabetically, this can also be anything else. \n",
    "trial_ids['Dataset'] = 'Scene'\n",
    "#ind = trial_ids['ID'].str.startswith('COCO')\n",
    "#print(ind)\n",
    "trial_ids.loc[trial_ids['ID'].str.contains('COCO'), 'Dataset']  = 'COCO'\n",
    "trial_ids.loc[trial_ids.ID.str.contains(r'n(0|1)[0-9][0-9][0-9][0-9]'), 'Dataset'] = 'ImageNet'\n",
    "\n",
    "#trial_ids_sorted = trial_ids.sort_values(by=['Dataset', 'ID']).reset_index()\n",
    "trial_ids_sorted = trial_ids.sort_values(by=['ID']).reset_index()\n",
    "print(trial_ids_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing and visualizing neural RDMs\n",
    "Now that we've organized the data, we can make both the individidual and the group RDMs. To do this, we need to use the trial indices we've just created and apply them to the patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = 4\n",
    "rois = data['CSI1'].keys()\n",
    "rois = [roi for roi in rois if roi.endswith(roi_key)]\n",
    "# preallocate for the rdms (subjects, regions, images, images)\n",
    "rdms = np.zeros((subs, len(rois), len(trial_ids_sorted), len(trial_ids_sorted )))\n",
    "for sub in range(1, subs + 1): # loop over subjects\n",
    "    for n, roi in enumerate(rois): # and ROIs\n",
    "        ids = list(trial_ids_sorted['CSI' + str(sub) + '_trials'].to_numpy(dtype='int'))\n",
    "        \n",
    "        sel_data = np.array(data['CSI' + str(sub)][roi])[ids]\n",
    "        print(sel_data.shape)\n",
    "        rdms[sub-1, n, :, : ] = 1 - np.round(np.corrcoef(sel_data),4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before storing the final RDMs we can visualized them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import rankdata\n",
    "fig, ax = plt.subplots(4,2, figsize=(4,10))\n",
    "\n",
    "for sub in range(subs):\n",
    "    for roi in range(len(rois)):\n",
    "        show_rankRDM(rdms[sub,roi], ax = ax[sub, roi], label=None)\n",
    "        if sub == 0:\n",
    "            ax[sub, roi].set_title(rois[roi])\n",
    "        if roi == 0:\n",
    "            ax[sub, roi].set_ylabel('Subject ' + str(sub+1) )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Compute and plot the grand average RDM. Store the GA rdm in a variable called GA.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8d5e89046cf478e74b967b48fb04dba",
     "grade": false,
     "grade_id": "cell-067583bfbccb888c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61ccc5877c6a90a4af6f749c8f309392",
     "grade": true,
     "grade_id": "cell-73fafff74f362cbc",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from week_lynn import test_GA\n",
    "\n",
    "test_GA(rdms, GA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9efb0167a9656313a9d12645823132a",
     "grade": true,
     "grade_id": "cell-944b1e722ca777c4",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): So we see that there is a lot of structure in this brain area in response to these diverese set of images! Most prominently, there is a big cluster in the middle of the RDM. What does it mean? Use the `trial_ids_sorted` dataframe and find out what these images are and why they might cluster! Explain what happens in the text field below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be661a3c724864bc1901a0f3914d434d",
     "grade": true,
     "grade_id": "cell-77c72e3ef2d688ea",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let's save the neural RDMs!\n",
    "if not os.path.isdir(os.path.join(data_dir, 'Results')):\n",
    "    os.mkdir(os.path.join(data_dir, 'Results'))\n",
    "np.save(os.path.join(data_dir, 'Results', 'RDM_LO.npy'), rdms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise ceiling\n",
    "\n",
    "As you can see in the RDMs above, our neural data is not perfect and there are quite some differences between the participants. This is a common problem, since any type of neural data consists of both signal and noise. Oftentimes, our definitions of signal are quite simply derived from our experimental design in a homogeneous group of participants, so that a signal is something that is shared across all trials of the same type or participants in the group (the statistical reason behind averaging across trials and subjects). This view also entails that anything that is not reliably shared across subject has to be defined as noise (e.g. unrelated to the studied process), because it cannot be captured with our methods, it is thus limiting our explanatory power. \n",
    "\n",
    "In the later parts of the tutorial, we will ask how well these neural RDMs are captured by the features of different computational models. To understand, how well any model could possibly do, we need to get a grasp on the reliable signal in this ROI. With RSA, this is typically done by constructing the upper and lower bound of the noise ceiling.\n",
    "\n",
    "**Upper noise ceiling bound**: This one is usually obtained by computing the correlation for a single participants RDM to the average RDM. This is a somewhat optimistic noise estimate since any single subject's RDM also contributed to the average RDM, for this reason it is the upper bound. No model can exceed this upper noise bound as it represents the best possible model.\n",
    "\n",
    "**Lower noise ceiling bound**: This one is obtained by computing the correlation for a single participants RDM to the average RDM without this individual participant. This is a measure of consistency across subjects. If the lower noise ceiling bound is close to zero, this means that there is no variance to be explained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (2 points): Let's construct the noise ceiling for our data! To this end, first average across hemisphere to obtain a single RDM per participant. Compute the upper and lower noise ceiling per participant and the average across all participants. For estimating the correlation, use Spearman's rho. Store the result `noiseCeiling['UpperBound']` and `noiseCeiling['LowerBound']`, respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47b00ce0308ed54fc023a722c0e6a999",
     "grade": false,
     "grade_id": "cell-41b471814322b24b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "from scipy.stats import spearmanr\n",
    "rdms_neural = np.load(os.path.join(data_dir, 'Results', 'RDM_LO.npy'))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d2f1fbd93cb4dd2390d6ae1f14b438c",
     "grade": true,
     "grade_id": "cell-d0cfd2e3da69df7d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from week_lynn import test_NC\n",
    "\n",
    "test_NC(rdms_neural, noiseCeiling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): Why do you think the span between the lower and the upper bound is so large?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "647b8f3cd3f7a476d14ecafb1b00a153",
     "grade": true,
     "grade_id": "cell-e4fa4d86fb7b99c9",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trials and reset workspace\n",
    "if not os.path.isdir(os.path.join(data_dir, 'Results')):\n",
    "    os.mkdir(os.path.join(data_dir, 'Results'))\n",
    "trial_ids_sorted.to_pickle(os.path.join(data_dir, 'Results', 'trial_ids.pkl'))\n",
    "try:\n",
    "    joblib.dump(noiseCeiling, os.path.join(data_dir, 'Results', 'NC_LO.pkl'))\n",
    "except:\n",
    "    # If you didn't succeed to compute the noise ceiling, you can download it here:\n",
    "    if not os.path.isfile(os.path.join(data_dir, 'Results', 'NC_LO.pkl')):\n",
    "        filename = os.path.join(data_dir, 'Results', 'NC_LO.pkl')\n",
    "        !wget -O $filename https://surfdrive.surf.nl/files/index.php/s/Lyco1827lUTgAWO/download\n",
    "print('Done saving!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the workspace!\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the first part of this tutorial! You have now organized the dataset across subjects into a single format and converted it into an RDM. In the next part, we will try to explain these neural RDMs by using different computational models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational models as hypotheses on neural processing\n",
    "\n",
    "In this part of the tutorial, we move our focus to computational models. These type of models can be loosely defined as an input-output mapping, that is, a certain operation is *computed* on the input and returns an output.\n",
    "\n",
    "In some ways, these computational models can be viewed as hypotheses of neural information processing. The idea is that if a model can explain neural activity either with its output or its process (computations), then there may be some overlap in computations between such a model and the brain. One example for a computational vision model is the Gabor Filter bank, where the input image is convolved with different gaussians of different phases and frequencies, giving rise to a set of more useful features for further visual processing. This rather simple model has been quite successful in predicting neural responses in V1 even though it is not the state-of-the-art anymore ([Cadena et al., 2019](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006897)).\n",
    "\n",
    "In recent years, more and more sophisticated computational models have entered neuroscience fueled by the recent progress in deep learning. In the next two parts of this tutorial, we will first look at a vision deep learning model and later at a natural language processing model. The overarching goal is to leverage these models to explain the neural activity in LOC. Like this, we can ask a couple of questions such as:\n",
    "\n",
    "* How well can a vision model actually capture visual processing in LOC?\n",
    "* Can a language model explain anything in this *visual* area?\n",
    "* Do the vision and language model explain the same variances in the data or are they accounting for different things?\n",
    "\n",
    "Please note that depending on our research question, we could have adopted many other computational models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep convolutional neural networks\n",
    "In this tutorial, we are going to focus on a single feedforward deep convolutional neural network architecture class, termed [VGG19](https://arxiv.org/abs/1409.1556). We chose this one for its simplicity in architecture, it is actually not the most successful one anymore (https://paperswithcode.com/sota/image-classification-on-imagenet). However, it is still the second to best models for explaining primate visual processing (http://www.brain-score.org/).\n",
    "\n",
    "It is called VGG*19* because it consists of 16 convolutional layers and 3 dense layers.\n",
    "\n",
    "Ok, as a first step let's load in our model. Deep learning libraries such as [tensorflow](https://www.tensorflow.org/) and [keras](https://keras.io/) make this very easy for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports for the rest of the tutorial\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input, decode_predictions\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import rankdata, spearmanr\n",
    "from niedu.utils.nipa import show_rankRDM\n",
    "sys.path.append('/home/Public')\n",
    "# This reduces the number of cpu cores used.\n",
    "jobs=7\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=jobs,\n",
    "                         inter_op_parallelism_threads=jobs,\n",
    "                         allow_soft_placement=True)\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model\n",
    "model = keras.applications.VGG19(weights='imagenet') # and specify that it should also load the pretrained weights!\n",
    "# print an overview!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output above tells us a number of important things about our model!\n",
    "\n",
    "A feedforward DCNN is a series of operations applied to an input image. You can see from the first row in the table states that this image is required to have the dimensions of `224, 224, 3`, where the first two numbers refer to the height and width of the image and the last one is the color channel (RGB, so 3). The first dimenion `None` here refers to the batch size, that is, the amount of images presented to the network. So if we were to give 5 images to the network, this would have the dimension of `(5, 224, 224, 3)`\n",
    "\n",
    "Further inspecting the table, you can see that Convolutions and MaxPooling operations are repeatedly applied to the image. \n",
    "This means that successively applying these operations to our input image gradually transforms it into a 1000-dimensional vector, the network's prediction!\n",
    "\n",
    "Finally at the bottom of the table, you can see the amount of free parameters that network has at its disposal during training for finding an effective mapping between the input image and the target label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): The right handside of the table specifies the amount of parameters at every layer of the network. Why do you think the amount of parameters grow with the increasing number of layers? Why do dense layer have so many more parameters compared to convolutional layers?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7aff62cf40f23f737382fe52acd724c6",
     "grade": true,
     "grade_id": "cell-45ea67227d1f3fcf",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's try out if this model actually works! To do this, we'll load in the stimuli used in the BOLD5000 experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the directories\n",
    "data_dir = os.path.join(os.path.expanduser('~'), 'NI-edu-data', 'BOLD5000')\n",
    "img_dir = os.path.join(data_dir, 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "print(\"We have the following folders:\\n-\", '\\n- '.join(sorted(os.listdir(img_dir))))\n",
    "# Pick one specific image)\n",
    "img_path = os.path.join(img_dir, 'ImageNet','n01532829_11283.JPEG')\n",
    "\n",
    "# Load this image\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# and visualize it!\n",
    "sns.set_context('poster')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('What kind of \"object\" is this?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we want to evaluate whether the model can recognize the object on this image. We can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first have to convert it from an image format to an array\n",
    "x = image.img_to_array(img)\n",
    "# Then add the batch dimension, in this case just 1 image.\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# and then we have to preprocess the image! (e.g. switch color channels, normalize etc.)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# finally, we can simply predict the image.\n",
    "predictions = model.predict(x)\n",
    "# This provides us with vector of 1000 entries, the softmax probability for every class.\n",
    "\n",
    "# we can use the function below to translate this vector into more interpretable prediction:\n",
    "print('Predicted:', decode_predictions(predictions , top=3)[0]) # Returns the top 3 predictions for the first image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this was a clearly a house finch! The model's prediction also show that the other 999 catgories did not stand a chance. The second most highly predicted class was only at 0.00045%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Load in a picture from the COCO database, display it and have the model predict this image. Does the model's response make sense? If so, why? If not, why not? Store the models response in a variable called `predictions` and use the image 'COCO_train2014_000000000036.jpg'.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b37ff33588d4626564316332b0d2e03",
     "grade": false,
     "grade_id": "cell-9407c2651f793791",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "img_path = os.path.join(img_dir, 'COCO','COCO_train2014_000000000036.jpg')\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f59bba46030a4ca6bb74b1cde315543d",
     "grade": true,
     "grade_id": "cell-b494380f6e757077",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from week_lynn import test_prediction\n",
    "\n",
    "test_prediction(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have established that the model can actually do its job, we can start to harness its features for understanding the neural data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction \n",
    "\n",
    "As outlined above, we want to use the VGG features to see whether they can account for our neural data from area LOC. To do this, we first have to translate the VGG model features into our common space, representational dissimilarity!\n",
    "Doing this with a DCNN is actually pretty comparable to running a neuroimaging experiment: You present the model/subject with a set of experimental images, you record its activations while its processing the images and probe it for its responses!\n",
    "\n",
    "This also means that we can easily construct RDMs for the VGG model if we can access its activations. This is what we are going to do next, **feature extraction**. We will first load in the experimental stimuli, then present these images to the model and record its activations in response to these images. Later, based on the activations, we will construct an RDM.\n",
    "\n",
    "As a first step, we need to decide where to extract the activations in the model. Let's take a look at an middle layer first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'block4_pool' # name of the layer of interest\n",
    "\n",
    "# create a new model, that end with the layer of interest.\n",
    "model_features = Model(inputs=model.input, outputs=model.get_layer(layer).output)\n",
    "# use this shortened model to evaluate the image\n",
    "features = model_features.predict(x)\n",
    "# the output are the intermediate features of the model!\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this for our house finch image, gives us a activations in the shape of `(1, 14, 14, 512)`, just like we would expect based on the summary table above. For constructing an RDM, we will flatten this matrix such that we obtain `(images, features)`, which is the product of all our former dimensions (`14*14*512`), so 100,352 features for every image!\n",
    "\n",
    "Most of the time, the actual information usually falls into much lower dimensional manifold. This means that that while the information is currently represented along so many dimensions, fewer dimensions could suffice to approximatethe same kind of informantion. Let's use PCA, a trick you learnt about in week 1, to reduce the amount of features. This will also help us to reduce the computational time for obtaining the pairwise distances for our model RDMs later on.\n",
    "\n",
    "To do this, we need to load in many more images and obtain their activations, because the dimensionality reduction in PCA is limited by the amount of samples available, so if we only have 5 images, we can only project our 100,352-dimensional space into a 5-dimensional one, which will likely lead to a lot of lost variance. So let's get more images!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load in some more images and pick those, we used in the neural RDM.\n",
    "trial_ids = pd.read_pickle(os.path.join(data_dir, 'Results', 'trial_ids.pkl'))\n",
    "\n",
    "# Let's pick a random 1000 images, around a third of our data \n",
    "# to get a good estimate of the dimensionality of the variance.\n",
    "n = 1000\n",
    "i_rand = np.random.randint(0, len(trial_ids), n) # let's pick a random set of 1000 images\n",
    "\n",
    "imgs = []\n",
    "for i in i_rand:\n",
    "    img_id = trial_ids.loc[i, 'ID']\n",
    "    if img_id.startswith('rep_'): # There are trials call rep_, which simply means that the same image was shown twice.\n",
    "        img_id = img_id[4:]\n",
    "          \n",
    "    imgs.append(image.img_to_array(image.load_img(os.path.join(img_dir, trial_ids.loc[i, 'Dataset'], img_id ), target_size=(224, 224))))\n",
    "    \n",
    "# and preprocess (same as we did for a single image!)\n",
    "imgs = preprocess_input(np.array(imgs)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the activations\n",
    "features = model_features.predict(imgs, verbose=True) # 1000 images --> ca. 2 - 2.3 GB RAM\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And apply the PCA transform!\n",
    "pca = PCA(n_components=500)\n",
    "features = np.reshape(features,(features.shape[0], -1)) # flatten, only maintaining the samples\n",
    "# Fit and transform the data \n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# Evaluate how the variance is distributed across the PCA components. \n",
    "plt.plot(np.arange(500),pca.explained_variance_ratio_ )\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Variance explained')\n",
    "plt.title('Total variance explained: ' + str(np.sum(pca.explained_variance_ratio_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so doing a PCA seems to be a reasonable way to reduce the dimensionality of the data. We traded around 100.000 for 500 features and maintained around 84% of the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 point): Why do you think we chose 1000 *random* images for fitting the PCA transform instead of just using images from ImageNet for instance? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c55bf15cd393123c88099acb3a578fcf",
     "grade": true,
     "grade_id": "cell-35dbbaef83db6ab1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing RDMs from DNN features\n",
    "In our next step, we will apply this transformation to all our images and then we can construct an RDM with this data. This will take a little while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our first 1000 features in a large matrix (images x features)\n",
    "features_pca_all = np.zeros((len(trial_ids), features_pca.shape[1]))\n",
    "print(features_pca_all.shape) \n",
    "\n",
    "# transfer the features that we're already transformed\n",
    "features_pca_all[i_rand] = features_pca\n",
    "\n",
    "# Now load in the remaining pictures in batches \n",
    "batches = np.linspace(0, len(trial_ids), 4, endpoint=True, dtype=int)\n",
    "for b in range(len(batches)-1):\n",
    "    print('Processing batch ' + str(b + 1))\n",
    "    imgs = []\n",
    "    i_seq = []\n",
    "    for i in range(batches[b], batches[b+1]):\n",
    "        if i not in i_rand:\n",
    "            i_seq.append(i)\n",
    "            img_id = trial_ids.loc[i, 'ID']\n",
    "            if img_id.startswith('rep_'):\n",
    "                img_id = img_id[4:]\n",
    "\n",
    "            imgs.append(image.img_to_array(image.load_img(os.path.join(img_dir, trial_ids.loc[i, 'Dataset'], img_id ), target_size=(224, 224))))\n",
    "\n",
    "    # and preprocess!\n",
    "    imgs = preprocess_input(np.array(imgs)) \n",
    "    \n",
    "    # and predict!\n",
    "    features = model_features.predict(imgs, verbose=True) \n",
    "    print(features.shape)\n",
    "\n",
    "    # transform into lower dimensional space\n",
    "    features_pca = pca.transform(np.reshape(features,(features.shape[0], -1)))\n",
    "    \n",
    "    # write to array!\n",
    "    features_pca_all[i_seq] = features_pca\n",
    "    \n",
    "    del features, features_pca, imgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with this we can now construct our RDM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = 1 - np.round(np.corrcoef(features_pca_all),4)  \n",
    "np.save(os.path.join(data_dir, 'Results', 'RDM_' + layer + '.npy'), rdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "show_rankRDM(rdm, label=layer,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you've just seen, the construction of an RDM is quite comparable in an DCNN! This model RDM even already looks a bit similar to our neural RDM. In our next step, we will compare it back to the RDM that we've computed for the region LOC and see how well this DCNN layer explains the neural RDMs of the four participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in our earlier result and the noise ceiling!\n",
    "layer = 'block4_pool'\n",
    "data_dir = os.path.join(os.path.expanduser('~'), 'NI-edu-data', 'BOLD5000')\n",
    "# model RDM\n",
    "rdm = np.load(os.path.join(data_dir, 'Results', 'RDM_' + layer + '.npy'))\n",
    "# neural RDM\n",
    "rdms_neural = np.load(os.path.join(data_dir, 'Results', 'RDM_LO.npy'))\n",
    "# Noise Ceiling\n",
    "noiseCeiling = joblib.load(os.path.join(data_dir, 'Results', 'NC_LO.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset contains only four subjects, we cannot model them as a random factor. Instead we use the mean RDM across participants, estimate variability by sampling across the stimuli and determine significance by creating a null-distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute relatedness\n",
    "mean_rdm = np.mean(np.mean(rdms_neural, axis=0),axis=0) # average over hemispheres and participans\n",
    "feature_rdm = squareform(rdm)\n",
    "rdm_corr = spearmanr(squareform(mean_rdm), feature_rdm)[0] # and compute the spearman corr to the feature RDM\n",
    "print(rdm_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance\n",
    "def corr_nullDist(mean_rdm, feature_rdm_vec, iterations=100):\n",
    "    rdm_corr_null = []\n",
    "    for i in range(iterations):\n",
    "        if i%10 == 0:\n",
    "            print('Iteration: ' + str(i) )\n",
    "        # Create a random index that respects the structure of an rdm.\n",
    "        shuffle = np.random.choice(mean_rdm.shape[0],mean_rdm.shape[0],replace=False)\n",
    "\n",
    "        # shuffle RDM consistently for both dims\n",
    "        mean_rdm_shuffle = mean_rdm[shuffle,:] # rows\n",
    "        mean_rdm_shuffle = mean_rdm_shuffle[:,shuffle] # columns\n",
    "\n",
    "        # correlating with neural similarty matrix\n",
    "        rdm_corr_null.append(spearmanr(squareform(mean_rdm_shuffle), feature_rdm_vec)[0])\n",
    "    return rdm_corr_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_corr_null = corr_nullDist(mean_rdm, feature_rdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot null distribution and true test statistic\n",
    "fig = sns.displot(rdm_corr_null,height=6,aspect=1.5)\n",
    "fig.set_axis_labels(\"Rhos\")\n",
    "plt.axvline(rdm_corr,color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute p-value\n",
    "p_val = np.mean(rdm_corr_null>rdm_corr) \n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 points): The p-value of our test-statistic is zero. Explain what that means with regard to the null distribution and how it can be zero. Would the p-value change if we ran 1000000 iterations instead of 100?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "933c72eac47b04bf8b8a2fb120ee26c1",
     "grade": true,
     "grade_id": "cell-1f81cc36dd5fc24e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we would like to assess how variable our measure of relatedness is. That is, just knowing how the mean subject RDM correlates with the feature RDM does not yet tell us, about the variability of this estimate. So let's estimate the confidence interval by using bootstrapping with replacement. Put simply, bootstrapping is a method in which you repeatedly sample from the same data. Importantly, you also put the data back after drawing and the same data points can be drawn multiple times within the same draw. Check out the example below:\n",
    "![Example for bootstrapping with replacement](https://www.researchgate.net/profile/Paola-Galdi/publication/322179244/figure/fig2/AS:588191077777408@1517247089079/An-example-of-bootstrap-sampling-Since-objects-are-subsampled-with-replacement-some.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping\n",
    "def corr_variability(mean_rdm_vec, feature_rdm_vec, iterations=100):\n",
    "\n",
    "    rdm_corr_boots = []\n",
    "    for i in range(iterations):\n",
    "        if i%10 == 0:\n",
    "            print('Iteration: ' + str(i) )\n",
    "        # Create a random index that respects the structure of an rdm.\n",
    "        sample = np.random.choice(mean_rdm_vec.shape[0],mean_rdm_vec.shape[0],replace=True) # note that now replace is True\n",
    "\n",
    "        # Subsample from both the reference and the feature RDM\n",
    "        mean_rdm_sample = mean_rdm_vec[sample] \n",
    "        feature_rdm_sample = feature_rdm_vec[sample] \n",
    "\n",
    "        # correlating with neural similarty matrix\n",
    "        rdm_corr_boots.append(spearmanr(mean_rdm_sample, feature_rdm_sample)[0])\n",
    "\n",
    "    return rdm_corr_boots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rdm_vec = squareform(mean_rdm)\n",
    "rdm_corr_boots = corr_variability(mean_rdm_vec, feature_rdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test statistics and bootstrapped distribution\n",
    "fig = sns.displot(rdm_corr_boots,height=6,aspect=1.5)\n",
    "fig.set_axis_labels(\"Rhos\")\n",
    "plt.axvline(rdm_corr,color=\"red\")\n",
    "# 95% confidence intervals:\n",
    "plt.axvline(np.percentile(rdm_corr_boots, 2.5), color='gray', ls='--') # 2.5%\n",
    "plt.axvline(np.percentile(rdm_corr_boots, 97.5), color='gray', ls='--') # 97.5% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToThink</b> (1 points): Please explain what the confidence interval tells us about the relatedness between our neural RDM and the feature RDM? What would a small and large confidence interval mean?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d757e9b0b4517bf9bf84e8065bca126d",
     "grade": true,
     "grade_id": "cell-7c19156d24ae140c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can assess how well this feature RDM fare with regard to the noise ceiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rdm_corr)/noiseCeiling['LowerBound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see that the model features from block 4 are actually accounting a sizable amount of the lower noise ceiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (3 points): Let's see how well a higher layer such as the first dense layer `fc1` can account for the data! Note that `fc1` is already 2 dimensional, so you can skip the PCA step! Don't forget to save the resulting RDM. Name this variable `rdm_fc1`. Start by loading in the images in batches, preprocess them, pass them to the feature model and obtain the features for all images. Once you've processed all images, compute the RDM and compare the feature RDM to the mean subject RDMs. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you start, let's clean up!\n",
    "del features_pca_all, rdm, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ea44fc303be33e3dc83258c72400191",
     "grade": false,
     "grade_id": "cell-bed27302df061ff5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "\n",
    "layer = 'fc1'\n",
    "model_features = Model(inputs=model.input, outputs=model.get_layer(layer).output)\n",
    "\n",
    "# Store all features in a large matrix (images x features)\n",
    "features_all = np.zeros((len(trial_ids), model.get_layer(layer).output_shape[-1]))\n",
    "print(features_all.shape) \n",
    "\n",
    "# Now load in the remaining pictures in batches \n",
    "batches = np.linspace(0, len(trial_ids), 4, endpoint=True, dtype=int)\n",
    "for b in range(len(batches)-1):\n",
    "    print('Processing batch ' + str(b + 1))\n",
    "    imgs = []\n",
    "    i_seq = []\n",
    "    \n",
    "    for i in range(batches[b], batches[b+1]): # Loop over images in the batch\n",
    "        i_seq.append(i)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fed612b4d04dcb5cf9fda0c062ef71d9",
     "grade": true,
     "grade_id": "cell-1163c3a5984f01b6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from week_lynn import test_fc1\n",
    "\n",
    "test_fc1(rdm_fc1, rdm_corr_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and save.\n",
    "try: \n",
    "    np.save(os.path.join(data_dir, 'Results', 'RDM_' + layer + '.npy'), rdm_fc1)\n",
    "except:\n",
    "    if not os.path.isfile(os.path.join(data_dir, 'Results', 'RDM_' + layer + '.npy')):\n",
    "        filename = os.path.join(data_dir, 'Results', 'RDM_' + layer + '.npy')\n",
    "        !wget -O $filename https://surfdrive.surf.nl/files/index.php/s/dmEjKGwQvtNJ1MN/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it again.\n",
    "fig, ax = plt.subplots()\n",
    "show_rankRDM(rdm_fc1, label=layer, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a moment to visualize our results! To do this, it's nice to also estimate the variability for the FC1 correlation first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_corr_boots_fc1 = corr_variability(mean_rdm_vec, squareform(rdm_fc1))\n",
    "fig = sns.displot(rdm_corr_boots_fc1,height=6,aspect=1.5)\n",
    "fig.set_axis_labels(\"Rhos\")\n",
    "plt.axvline(rdm_corr_fc1,color=\"red\")\n",
    "# Confidence inquantilervals:\n",
    "plt.axvline(np.percentile(rdm_corr_boots_fc1, 2.5), color='gray', ls='--') # 2.5%\n",
    "plt.axvline(np.percentile(rdm_corr_boots_fc1, 97.5), color='gray', ls='--') # 97.5% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all data in a single dataframe, useful for plotting.\n",
    "\n",
    "rhos = pd.DataFrame({'Features': np.repeat(np.arange(2)[np.newaxis,:],100,axis=1).flatten(),\n",
    "                          'Spearman rho': np.zeros(200)})\n",
    "# Assign the results per layer\n",
    "rhos.loc[rhos['Features'] == 0, 'Spearman rho'] = rdm_corr_boots \n",
    "rhos.loc[rhos['Features'] == 1, 'Spearman rho'] = rdm_corr_boots_fc1\n",
    "\n",
    "rhos['Features'].replace({0: 'Block4', 1: 'FC1'}, inplace=True)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(4,5))\n",
    "# Show each observation with a scatterplot\n",
    "sns.stripplot(x=\"Features\", y=\"Spearman rho\",\n",
    "              data=rhos, dodge=True, alpha=.5, zorder=1)\n",
    "\n",
    "# Show the conditional means\n",
    "sns.pointplot(x=\"Features\", y=\"Spearman rho\", \n",
    "              data=rhos, dodge=.532, join=False, palette=\"dark\",\n",
    "              markers=\"o\", scale=.25, ci=None)\n",
    "ax.set_ylim([0, noiseCeiling['LowerBound'] + 0.005])\n",
    "ax.axhline(noiseCeiling['LowerBound'], color='gray', label='lower NC')\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "ax.legend(loc=(1.04, 0), frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos.to_pickle(os.path.join(data_dir, 'Results','NeuralFits.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, well done! In this part, you've learnt how to leverage the statistical knowledge captured in DCNNs to explain neural processing in the area LOC. In the next section, we are going to be looking at a completely different kind of model, semantic embedding spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the workspace!\n",
    "%reset -f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic embedding spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic word embeddings are based on the idea that words that co-occur in a text are likely to be semantically related. For example, consider two news paper articles, one describing a trial in court and another one describing the birth of a baby panda in the zoo. If you now imagine the types of words that will occur in each of these article and then I would ask you if the word 'zebra' is more likely in the first or second article? Most likely, you would answer the second article. A semantic embedding space trained on large corpuses (collection of texts) such as news items is capturing this intuition. \n",
    "\n",
    "The idea of co-occurrence as a proxy for semnatic relatedness has fuelled progress in Natural Language Processing (NLP) and has led to the development of embedding spaces. There are both different kinds of models as well as datasets used for creating these semantic embedding spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import rankdata, spearmanr\n",
    "from niedu.utils.nipa import corr_variability\n",
    "sys.path.append('/home/Public')\n",
    "sns.set_context('poster')\n",
    "data_dir = os.path.join(os.path.expanduser('~'), 'NI-edu-data', 'BOLD5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This refers to that it was a glove model, trained on the wikipedia corpus with a 100 dimensional output space.\n",
    "# For an overview of all available models, see https://github.com/RaRe-Technologies/gensim-data\n",
    "embedding = \"glove-wiki-gigaword-100\" \n",
    "model = api.load(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now downloaded the pre-trained model, let's see what we can do with that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can give it a word and ask which other words are the most similar\n",
    "model.most_similar(positive = ['zebra'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you would expect, we see other animals being very similar to zebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can even do some more intricate stuff such as\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This  means, return words that are most similar to woman and king, while being dissimilar from the word man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most important feature for us is that we can ask how far apart in this 300-dimensional space two words are:\n",
    "print('Zebra and court: ' + str(model.distance('zebra', 'court')))\n",
    "print('Zebra and zoo: ' + str(model.distance('zebra', 'zoo')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also give multiple words and evaluate them with regard to their distance to a single word:\n",
    "model.distances('zebra', ['dog', 'body', 'frog', 'neuron'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Let's do some vector arithmetics! Come up with your own example of an analogy as we've seen above for queen example.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2043da42e1ae8054c7ada1cca17b8094",
     "grade": true,
     "grade_id": "cell-1e8d35ef690ff000",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature allows us to construct an RDM from our image labels! \n",
    "\n",
    "In the next step, we will load in the labels of our images and compute the RDMs for this embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to load in all the label information.\n",
    "trial_ids = pd.read_pickle(os.path.join(data_dir, 'Results', 'trial_ids.pkl'))\n",
    "data_dir = os.path.join(os.path.expanduser('~'), 'NI-edu-data', 'BOLD5000')\n",
    "coco_ann = joblib.load(os.path.join(data_dir,'BOLD5000_Stimuli', 'Image_Labels', 'coco_final_annotations.pkl'))\n",
    "\n",
    "coco_cat = open(os.path.join(data_dir,'BOLD5000_Stimuli', 'Image_Labels', 'coco-labels-paper.txt'), \"r\").read().split('\\n')\n",
    "imagenet_cat = open(os.path.join(data_dir,'BOLD5000_Stimuli', 'Image_Labels', 'imagenet_final_labels.txt'), \"r\").read().split('\\n')\n",
    "scene_cat = pd.read_csv(os.path.join(data_dir,'BOLD5000_Stimuli', 'Image_Labels', 'scene_final_labels_corrected.txt'), index_col=0)\n",
    "\n",
    "print(trial_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assign a label to every image!\n",
    "for ID in range(len(trial_ids)):\n",
    "    if trial_ids.loc[ID, 'ID'].startswith('rep_'):\n",
    "        tag = trial_ids.loc[ID, 'ID'][4:]\n",
    "    else:\n",
    "        tag = trial_ids.loc[ID, 'ID']\n",
    "    \n",
    "    # There is a different procedure for obtaining the labels depending on the dataset.\n",
    "    if trial_ids.loc[ID, 'Dataset'] == 'COCO':\n",
    "        annot = coco_ann[int(tag[-15:-4])] # We first have to strip the ID from the start and the ending, so that only the image identifier is maintained.\n",
    "        label = []\n",
    "        for a in range(len(annot)): # Some images contain multiple objects\n",
    "            label.append(coco_cat[annot[a]['category_id']-1]) # For every object, find the category id!\n",
    "        trial_ids.loc[ID, 'Label'] = ', '.join(label) # and link it back to the actual label.\n",
    "        \n",
    "    elif trial_ids.loc[ID, 'Dataset'] == 'Scene': # For the scenes, the label is contained in the ID.\n",
    "        label = ''.join(c for c in tag.split('.')[0] if not c.isdigit())\n",
    "        trial_ids.loc[ID, 'Label'] = scene_cat.loc[scene_cat['original category'] == label, 'corrected category'].item() \n",
    "        \n",
    "    elif trial_ids.loc[ID, 'Dataset'] == 'ImageNet': # For ImageNet, the label ID is contained in the ID.\n",
    "        label = tag.split('.')[0].split('_')[0] #\n",
    "        trial_ids.loc[ID, 'Label'] = [i for i in imagenet_cat if i.startswith(label)][0][10:]        \n",
    "    else:\n",
    "        print('Dataset not found.')\n",
    "        \n",
    "    print(trial_ids.loc[ID, 'Label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing RDMs from embedding spaces\n",
    "Now that we've assigned a label, that is, some description about what's on the picture, we can ask, how closely related are these different images semantically!\n",
    "\n",
    "We can construct an RDM by computing the pairwise distances between every set of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = np.ones((len(trial_ids),len(trial_ids))) # preallocate, note the ones since we are computing similarities now.\n",
    "\n",
    "for i in range(len(trial_ids)): # Loop over all images\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    label1 = trial_ids.loc[i,'Label'].lower().split(', ') # Prepare the label for the model.\n",
    "    \n",
    "    label1_clean = []\n",
    "    for word in label1:\n",
    "        if ' ' in word: # Some two word expressions, can be found as word1_word2 in the model corpus\n",
    "            word = word.replace(' ', '_') \n",
    "                \n",
    "        if word in model.key_to_index: # check if the label items are in the vocabulary of the model.\n",
    "            label1_clean.append(word)\n",
    "        else:\n",
    "            for w in word.split('_'): # if the words are not in the vocabulary, try to split them up again, to get a match.\n",
    "#                 if w in model.vocab:\n",
    "                if w in model.key_to_index:\n",
    "                    label1_clean.append(w) \n",
    "    \n",
    "    for j in range(i, len(trial_ids)):  # we can do this, since it's symmetric!\n",
    "        if len(label1_clean) !=0:\n",
    "            label2 = trial_ids.loc[j,'Label'].lower().split(', ')\n",
    "\n",
    "            label2_clean = []\n",
    "            for word in label2:\n",
    "                if ' ' in word:\n",
    "                    word = word.replace(' ', '_')\n",
    "#                 if word in model.vocab:\n",
    "                if word in model.key_to_index:\n",
    "                    label2_clean.append(word)\n",
    "                else:\n",
    "                    for w in word.split('_'):\n",
    "#                         if w in model.vocab:\n",
    "                        if w in model.key_to_index:\n",
    "                            label2_clean.append(w) \n",
    "\n",
    "            if len(label2_clean) !=0:\n",
    "            \n",
    "                rdm[i, j] = model.n_similarity(label1_clean, label2_clean) # compute the similarity between the two label sets.\n",
    "            else:\n",
    "                rdm[i, j] = np.nan\n",
    "        else:\n",
    "            #print(label1)\n",
    "            rdm[i, j] = np.nan\n",
    "        \n",
    "        rdm[j, i] = rdm[i, j]\n",
    "    \n",
    "    \n",
    "np.save(os.path.join(data_dir, 'Results', 'RDM_' + embedding + '.npy'), rdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(1 - rdm)\n",
    "plt.title(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above, you can see that the model corpus didn't know all the labels we've asked it for (white stripes & nan in the rdm). In fact, there are also some labels, you've probably never heard about yourself. Crucially, this language model can only process words that were contained in the corpus (collection of texts) that is was trained on.\n",
    "\n",
    "So now we have obtained the RDM based on the 100-dimensional embedding space! However, computing an RDM based on a larger corpus requires a lot of RAM. To this end, we've precomputed an RDM based on the GoogleNews corpus and the 300 dimensional word2vec embedding space for you. You can load in the RDM in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = 'word2vec-google-news-300'\n",
    "if not os.path.isfile(os.path.join(data_dir, 'Results', 'RDM_' + embedding + '.npy')):\n",
    "    filename = os.path.join(data_dir, 'Results', 'RDM_' + embedding + '.npy')\n",
    "    !wget -O $filename https://surfdrive.surf.nl/files/index.php/s/AVb4Z843OJRyOOZ/download\n",
    "rdm_w2v = np.load(os.path.join(data_dir, 'Results', 'RDM_' + embedding + '.npy'))\n",
    "\n",
    "plt.imshow(1 - rdm_w2v)\n",
    "plt.title(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, these two RDMs seem quite comparable yet if we correlate them, it becomes clear that they are by no means identical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to exclude all RDM entries that were not contained in either corpus.\n",
    "nan_filter = (np.isnan(rdm_w2v)[0,:] == False) & (np.isnan(rdm)[0,:] == False) \n",
    "rdm_filtered_w2v = squareform(1- np.round((rdm_w2v[nan_filter, :][:,nan_filter]),5))\n",
    "rdm_filtered_glove = squareform(1 - np.round((rdm[nan_filter, :][:,nan_filter]),5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6.5, 6.5))\n",
    "sns.scatterplot(x=rdm_filtered_w2v, y=rdm_filtered_glove, zorder=1, s=2)\n",
    "ax.plot([0, 1.5], [0, 1.5], linewidth = 5, c='gray', ls='--',zorder=4 )\n",
    "ax.set_xlabel('Word2vec')\n",
    "ax.set_ylabel('GloVe')\n",
    "ax.set_title(spearmanr(rdm_filtered_w2v, rdm_filtered_glove))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <b>ToDo</b> (1 point): As you can see above, the word embeddings are similar but not the same. Please name one reason for why the embedding spaces are different and explain why that is.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "566ca11ddb601e903ea0c590e8f33a50",
     "grade": true,
     "grade_id": "cell-44bac823893f2129",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (3 point): Take these language-based RDMs and link the, back to the neural RDM. Compute the correlation and estimate the variability! How many percent of the lower noise ceiling can be accounted for on average by the word2vec and the GloVe model? Store your answer in the variable `prop_explained_w2v` and `prop_explained_GloVe` for the correlation and report the 95% confidence interval in a vector `CI_prop_explained_w2v` and `CI_prop_explained_GloVe`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94874ea2fa0422227c6af563b4048924",
     "grade": false,
     "grade_id": "cell-b3fd440c784662fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "# Let's load in our earlier result and the noise ceiling!\n",
    "np.random.seed(3)\n",
    "\n",
    "rdms_neural = np.load(os.path.join(data_dir, 'Results', 'RDM_LO.npy'))\n",
    "noiseCeiling = joblib.load(os.path.join(data_dir, 'Results', 'NC_LO.pkl'))\n",
    "\n",
    "rdm_neural = np.mean(np.mean(rdms_neural, axis=0),axis=0)\n",
    "\n",
    "rdm_corrs = {}\n",
    "rdm_corrs_boots={}\n",
    "for embedding in ['word2vec-google-news-300','glove-wiki-gigaword-100']:\n",
    "    rdm_embedding = np.load(os.path.join(data_dir, 'Results', 'RDM_' + embedding + '.npy'))\n",
    "    # Since there are a couple of nan's similarities in our labels we have to exclude these before proceeding:\n",
    "    nan_filter = np.isnan(rdm_embedding)[0,:] == False\n",
    "    rdm_filtered = 1 - np.round((rdm_embedding[nan_filter, :][:,nan_filter]),5)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48aba2e519b0d126df684e067ccfa854",
     "grade": true,
     "grade_id": "cell-1eed8eab83f1d10f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from week_lynn import test_embeddings\n",
    "\n",
    "test_embeddings(prop_explained_w2v,prop_explained_GloVe, CI_prop_explained_w2v, CI_prop_explained_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize this and compare it back to the other features\n",
    "for embedding in ['word2vec-google-news-300','glove-wiki-gigaword-100']:\n",
    "    rhos = pd.read_pickle(os.path.join(data_dir, 'Results','NeuralFits.pkl'))\n",
    "    \n",
    "    new_rhos = pd.DataFrame({'Features': np.zeros(100),\n",
    "                          'Spearman rho': np.zeros(100)})\n",
    "\n",
    "    new_rhos.loc[new_rhos['Features'] == 0, 'Spearman rho'] = rdm_corrs_boots[embedding]\n",
    "\n",
    "    new_rhos['Features'].replace({0: embedding.split('-')[0]}, inplace=True)\n",
    "\n",
    "    rhos_updated = pd.concat([rhos, new_rhos])\n",
    "    rhos_updated.to_pickle(os.path.join(data_dir, 'Results','NeuralFits.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos_updated = pd.read_pickle(os.path.join(data_dir, 'Results','NeuralFits.pkl'))\n",
    "f, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "# Show each observation with a scatterplot\n",
    "sns.stripplot(x=\"Features\", y=\"Spearman rho\",\n",
    "              data=rhos_updated, dodge=True, alpha=.5, zorder=1)\n",
    "\n",
    "# Show the conditional means\n",
    "sns.pointplot(x=\"Features\", y=\"Spearman rho\", \n",
    "              data=rhos_updated, dodge=.532, join=False, palette=\"dark\",\n",
    "              markers=\"o\", scale=.25, ci=None)\n",
    "\n",
    "ax.set_ylim([0, noiseCeiling['LowerBound'] + 0.005])\n",
    "ax.axhline(noiseCeiling['LowerBound'], color='gray', label='lower NC')\n",
    "plt.tight_layout()\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in sum, this tells us that even a language model, so a model that has never learnt from anything *visual* can account for some of the patterns in the area LOC. Our finding is also supported by more recent studies, if you'd like to read more about this:\n",
    "* [Object representations in the human brain reflect the cooccurrence statistics of vision and language](https://www.biorxiv.org/content/10.1101/2020.03.09.984625v1.full.pdf)\n",
    "* [Learning as unsupervised alignment of conceptual systems](http://arxiv.org/abs/1906.09012)\n",
    "\n",
    "We also see that the word2vec model was much better than the GloVe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the workspace!\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining features spaces\n",
    "\n",
    "You might have realized that the VGG19 RDMs and the RDM we've obtained from the word2vec model do resemble one another. In this final part, we are going to explore in how far these models explain the *same* variance by combining them into a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import rankdata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from niedu.utils.nipa import show_rankRDM\n",
    "from niedu.utils.nipa import corr_variability\n",
    "sns.set_context('poster')\n",
    "sys.path.append('/home/Public')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in all our RDMs \n",
    "data_dir = os.path.join(os.path.expanduser('~'), 'NI-edu-data', 'BOLD5000')\n",
    "\n",
    "rdms_neural = np.load(os.path.join(data_dir, 'Results', 'RDM_LO.npy'))\n",
    "rdm_neural = np.mean(np.mean(rdms_neural, axis=0),axis=0)\n",
    "noiseCeiling = joblib.load(os.path.join(data_dir, 'Results', 'NC_LO.pkl'))\n",
    "rdm_w2v = np.load(os.path.join(data_dir, 'Results', 'RDM_' + 'word2vec-google-news-300' + '.npy'))\n",
    "rdm_pool4 = np.load(os.path.join(data_dir, 'Results', 'RDM_block4_pool.npy'))\n",
    "rdm_fc1 = np.load(os.path.join(data_dir, 'Results', 'RDM_fc1.npy'))\n",
    "\n",
    "# Since there are a couple of nan's similarities in our labels we have to exclude these before proceeding:\n",
    "nan_filter = np.isnan(rdm_w2v)[0,:] == False\n",
    "rdm_w2v_filtered = 1 - np.round((rdm_w2v[nan_filter, :][:,nan_filter]),5)\n",
    "rdm_pool4_filtered = np.round((rdm_pool4[nan_filter, :][:,nan_filter]),5)\n",
    "rdm_fc1_filtered = np.round((rdm_fc1[nan_filter, :][:,nan_filter]),5)\n",
    "rdm_neural_filtered = np.round((rdm_neural[nan_filter, :][:,nan_filter]),5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize = (11,8))\n",
    "ax = ax.flatten()\n",
    "show_rankRDM(rdm_pool4_filtered, label='VGG19 - pool4', ax=ax[0])\n",
    "show_rankRDM(rdm_fc1_filtered, label='VGG19 - FC1', ax=ax[1])\n",
    "show_rankRDM(rdm_w2v_filtered, label='Word2Vec', ax=ax[2])\n",
    "show_rankRDM(rdm_neural_filtered, label='Mean neural RDM', ax=ax[3])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>ToDo</b> (1 point): Last week, you've learnt about a way of combining different feature spaces! In the next step, please combine all our model RDMs into a single model to predict the mean participant RDM. Store the correlation in a variable called corr_combined and also determine the variability of this estimate with the CI.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45c19a6c81dd017d2d8c11f603491383",
     "grade": false,
     "grade_id": "cell-6e7384a68ed95286",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Implement your ToDo here. '''\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52955a9d7a0f6a8446190ea617ab35a9",
     "grade": true,
     "grade_id": "cell-24a5a768b00b8efb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "raises-exception",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "from week_lynn import test_multiFit\n",
    "\n",
    "test_multiFit(corr_combined, corr_combined_CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos = pd.read_pickle(os.path.join(data_dir, 'Results','NeuralFits.pkl'))\n",
    "\n",
    "new_rhos = pd.DataFrame({'Features': np.zeros(100),\n",
    "                          'Spearman rho': rdm_corrs_combined_boots})\n",
    "\n",
    "new_rhos['Features'].replace({0: 'Combined'}, inplace=True)\n",
    "\n",
    "rhos_updated = pd.concat([rhos, new_rhos])\n",
    "rhos_updated.to_pickle(os.path.join(data_dir, 'Results','NeuralFits.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize this!\n",
    "f, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Show each observation with a scatterplot\n",
    "sns.stripplot(x=\"Features\", y=\"Spearman rho\",\n",
    "              data=rhos_updated, dodge=True, alpha=.5, zorder=1)\n",
    "\n",
    "# Show the conditional means\n",
    "sns.pointplot(x=\"Features\", y=\"Spearman rho\", \n",
    "              data=rhos_updated, dodge=.532, join=False, palette=\"dark\",\n",
    "              markers=\"o\", scale=.25, ci=None)\n",
    "\n",
    "\n",
    "ax.set_ylim([0, noiseCeiling['LowerBound'] + 0.005])\n",
    "ax.axhline(noiseCeiling['LowerBound'], color='gray', label='lower NC')\n",
    "plt.tight_layout()\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "In this tutorial, you have learnt how to describe the variation in a given brain area in response to images by leveraging different computational models. There are several things we can conclude from this:\n",
    "\n",
    "* Different computational models can pick up on similar or different variation in your neural data. This can be seen from our last result in which the combined model is better than the other ones by themselves. If there is a lot of unique variance in the neural data that is accounted for by the individual models, then their combination will result in an even better explanation of the neural data. If however the different features explain the *same* variation in the data, then combining them will not yield a better result.\n",
    "* A language model can actually explain a visual area reasonably well, which tells us that linguistic co-occurrences and visual features might be related!\n",
    "* Instead of fitting a single GLM, we could have also partioned the variances accounted for by every set of features. That is, we could have described how much of the neural data is uniquely explained by a particular set of features, while controlling for the influence of the other features (this is called partial correlation). For an example of this approach, see this [paper](http://dx.doi.org/10.7554/eLife.32962).\n",
    "* This tutorial largely omitted the issue of correcting for multiple comparisons. Especially, when comparing multiple models to brain data, it is recommended to correct for the false discovery rate when interpreting the statistical signficance of the model fits.\n",
    "* Finally, all our resampling implementations (CI & stat. sig.) only used a 100 iterations per default to keep computation times manageable. For your own research, you should run at least multple thousands of iterations to get reliable estimates. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
